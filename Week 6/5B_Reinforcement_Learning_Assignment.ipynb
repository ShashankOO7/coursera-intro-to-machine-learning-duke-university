{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Assignment: Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Abhinav Tembulkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. One of the fundamental challenges of reinforcement learning is balancing *exploration* versus *exploitation*. What do these two terms mean, and why do they present a challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exploration**- \n",
    "1. Here we try different actions, we might learn something from we didn't expect from optimal actions generated from our estimates.\n",
    "2. Here we take actions which are NOT OPTIMAL based on our current estimate of Q function, We take actions we never took before.\n",
    "<br>\n",
    "\n",
    "**exploitation**- \n",
    "1. Here we are going to exploit Q function we possess and we take actions that are optimal for a given state.\n",
    "2. We exploit our learnt experience or learnt Q function. Always taking actions that maximize the Q function at any state.\n",
    "\n",
    "These two terms present a challenge as they are goals of opposite nature.A exploration strategy often opposes exploitation strategy and vice-versa.Yet anyy model needs both characteristics to achieve its goal utlimately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Another fundamental reinforcement learning challenge is what is known as the *credit assignment problem*, especially when rewards are sparse. What do we mean by the phrase, and why does it make learning especially difficult?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit assginment problem or CAP is a problem that occurs in context of reinfocemtn learning where an agent interactiong with an environment takes action such that to maximise total reward or minize temporal difference.\n",
    "\n",
    "When rewards are sparse , agent would find it difficult to reduce temporal difference frequently as a result of which taking actions is dfficult as actions are taken by an agent based on rewards it recieves on its actions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep SARSA Cart Pole\n",
    "\n",
    "[SARSA (state-action-reward-state-action)](https://en.wikipedia.org/wiki/Stateâ€“actionâ€“rewardâ€“stateâ€“action) is another Q value algorithm that resembles Q-learning quite closely:\n",
    "\n",
    "Q-learning update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma \\max_a Q_\\pi(s_{t+1}, a)\\big)\n",
    "\\end{equation}\n",
    "\n",
    "SARSA update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma Q_\\pi(s_{t+1}, a_{t+1})\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Unlike Q-learning, which is considered an *off-policy* network, SARSA is an *on-policy* algorithm. \n",
    "When Q-learning calculates the estimated future reward, it must \"guess\" the future, starting with the next action the agent will take. In Q-learning, we assume the agent will take the best possible action: $\\max_a Q_\\pi(s_{t+1}, a)$. SARSA, on the other hand, uses the action that was actually taken next in the episode we are learning from: $Q_\\pi(s_{t+1}, a_{t+1})$. In other words, SARSA learns from the next action he actually took (on policy), as opposed to what the max possible Q value for the next state was (off policy).\n",
    "\n",
    "Build an RL agent that uses SARSA to solve the Cart Pole problem. \n",
    "\n",
    "*Hint: You can and should reuse the Q-Learning agent we went over earlier. In fact, if you know what you're doing, it's possible to finish this assignment in about 30 seconds.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solution to the above aasignment below\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(4,24)\n",
    "    self.fc2 = nn.Linear(24,48)\n",
    "    self.fc3 = nn.Linear(48,2)\n",
    "\n",
    "  def forward(self,X):\n",
    "    x = F.relu(self.fc1(X))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    y = self.fc3(x)\n",
    "    return y\n",
    "\n",
    "EPISODES = 1000\n",
    "SUCCESS_TICKS = 195\n",
    "GAMMA = 1.0\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "ALPHA = 0.01\n",
    "ALPHA_DECAY = 0.01\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class DQNcartpolesolver:\n",
    "  def __init__(self,max_env_steps=None,monitor=False,quiet=True):\n",
    "    self.memory = deque(maxlen=100000)\n",
    "    self.env = gym.make('CartPole-v0')\n",
    "    if monitor : \n",
    "      self.env = gym.wrappers.Monitor(self.env,'../data/cartpole-1',force=True)\n",
    "    self.quiet = quiet\n",
    "    self.gamma = GAMMA\n",
    "    self.epsilon = EPSILON\n",
    "    self.epsilon_min = EPSILON_MIN\n",
    "    self.epsilon_decay = EPSILON_DECAY\n",
    "    self.alpha = ALPHA\n",
    "    self.alpha_decay = ALPHA_DECAY\n",
    "    self.n_episodes = EPISODES\n",
    "    self.n_win_ticks = SUCCESS_TICKS\n",
    "    self.batch_size = BATCH_SIZE\n",
    "    if max_env_steps is not None:\n",
    "      self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "    self.dqn = DQN()\n",
    "    self.criterion = nn.MSELoss()\n",
    "    self.opt = torch.optim.Adam(self.dqn.parameters(),lr=0.01)\n",
    "\n",
    "  def get_epsilon(self,t):\n",
    "    return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "  def preprocess_state(self,state):\n",
    "    return torch.tensor(np.reshape(state,[1,4]),dtype=torch.float32)\n",
    "\n",
    "  def choose_action(self,state,epsilon):\n",
    "    if np.random.random()<=epsilon:\n",
    "      return self.env.action_space.sample()\n",
    "    else : \n",
    "      with torch.no_grad(): return torch.argmax(self.dqn(state)).numpy()\n",
    "\n",
    "  def remember(self,state,action,reward,next_state,done,next_action):\n",
    "    reward = torch.tensor(reward)\n",
    "    self.memory.append((state, action, reward, next_state, done,next_action))\n",
    "\n",
    "  def replay(self,batch_size):\n",
    "    y_batch, y_target_batch = [], []\n",
    "    minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "    for state, action, reward, next_state, done,next_action in minibatch:\n",
    "      y = self.dqn(state)\n",
    "      y_target = y.clone().detach()\n",
    "      with torch.no_grad():\n",
    "        y_target[0][action] = reward if done else reward + self.gamma*self.dqn(next_state)[0][next_action]\n",
    "      y_batch.append(y[0])\n",
    "      y_target_batch.append(y_target[0])\n",
    "  \n",
    "    y_batch = torch.cat(y_batch)\n",
    "    y_target_batch = torch.cat(y_target_batch)\n",
    "    \n",
    "    self.opt.zero_grad()\n",
    "    loss = self.criterion(y_batch, y_target_batch)\n",
    "    loss.backward()\n",
    "    self.opt.step()        \n",
    "    \n",
    "    if self.epsilon > self.epsilon_min:\n",
    "      self.epsilon *= self.epsilon_decay\n",
    "\n",
    "  def run(self):\n",
    "    scores = deque(maxlen=100)\n",
    "\n",
    "    for e in tqdm(range(self.n_episodes)):\n",
    "      state = self.preprocess_state(self.env.reset())\n",
    "      done = False\n",
    "      i = 0\n",
    "    \n",
    "      action = self.choose_action(state, self.get_epsilon(e))\n",
    "    \n",
    "      while not done:\n",
    "        if e % 100 == 0 and not self.quiet:\n",
    "            self.env.render()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        next_state = self.preprocess_state(next_state)\n",
    "\n",
    "        next_action = self.choose_action(next_state,self.get_epsilon(e))\n",
    "        self.remember(state, action, reward, next_state, done,next_action)\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        i += 1\n",
    "\n",
    "        scores.append(i)\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score >= self.n_win_ticks and e >= 100:\n",
    "            if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "            return e - 100\n",
    "        if e % 100 == 0 and not self.quiet:\n",
    "            print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        self.replay(self.batch_size)\n",
    "    \n",
    "    if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    agent = DQNcartpolesolver()\n",
    "    agent.run()\n",
    "    agent.env.close()\n",
    "    \n",
    "import time\n",
    "\n",
    "def game (self):\n",
    "    # lets game\n",
    "    state = self.env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      while not done:\n",
    "        state = torch.Tensor([state]).view(-1,4)\n",
    "        y = self.dqn(state)\n",
    "        action = torch.Tensor([torch.argmax(y)]).view(-1,1)[0]\n",
    "        print(y,action)\n",
    "        obs = self.env.step(int(action))\n",
    "        next_state,rwrd,done,_ = obs\n",
    "        state = next_state\n",
    "        self.env.render()\n",
    "        reward+=rwrd\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    self.env.close()\n",
    "    print(\"YOUR REWARD: \",reward)\n",
    "    \n",
    "game(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
